{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trenowanie modelu\n",
    "\n",
    "Wybrałam zadanie Emotion and sentiment recognition. W tym notatniku opisuję trenowanie modeli i pokazuję ich wyniki. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie do pracy\n",
    "\n",
    "Ponieważ trenowałam na własnym komputerze a nie na Colabie, to upewniam się że karta graficzna jest dostępna i ustawiam ją jako domyślnie używane urządzenie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU NVIDIA GeForce GTX 1660 Ti will be used.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    print(f'GPU {torch.cuda.get_device_name(0)} will be used.')\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usuwam cache, żeby podczas trenowania nie dostawać błędu CUDA_out_of_memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych\n",
    "\n",
    "Wcześniej sklonowałam z githuba repozytorium z danymi, wczytuję odpowiednie pliki tutaj:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('2024-emotion-recognition/train/in.tsv', encoding='utf-8') as file:\n",
    "  X_train = pd.read_csv(file, sep='\\t')\n",
    "\n",
    "with open('2024-emotion-recognition/train/expected.tsv', encoding='utf-8') as file:\n",
    "  Y_train = pd.read_csv(file, sep='\\t')\n",
    "  Y_train = Y_train.astype(float)\n",
    "\n",
    "sentences = X_train.text.values\n",
    "labels = Y_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapisuję nazwy etykiet i przypisuję im ID, żeby przekazać potem te informacje modelowi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = Y_train.columns\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for i, name in enumerate(colnames):\n",
    "    id2label[i] = name\n",
    "    label2id[name] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ładuję tokenizer od modelu twitter-sentiment-pl-base (https://huggingface.co/bardsai/twitter-sentiment-pl-base). Jest to model do analizy sentymentu, który znalazłam na HuggingFace, był trenowany na danych z twittera z TweetEval (Barbieri i in. 2020, link: https://aclanthology.org/2020.findings-emnlp.148.pdf) przetłumaczonych na język polski. Wydaje mi się, że te dane treningowe są dość podobne do tego co jest przetwarzane w tym projekcie, więc liczę że to pozytywnie wpłynie na wyniki modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\magda\\kogni\\inzynieria lingwistyczna\\zaliczenie\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %pip install -U sacremoses # ta paczka była potrzebna do używania HerbertTokenizer\n",
    "from transformers import HerbertTokenizer\n",
    "\n",
    "tokenizer = HerbertTokenizer.from_pretrained('bardsai/twitter-sentiment-pl-base', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzam maksymalną długość zdania w zbiorze danych, ale trenowanie na tak długich inputach zajmuje zbyt dużo czasu, więc później ograniczyłam to do 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  397\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    input_ids = tokenizer.encode(sent)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robię encoding dla zdań w zestawie treningowym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\magda\\kogni\\inzynieria lingwistyczna\\zaliczenie\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sent,\n",
    "        add_special_tokens = True,\n",
    "        max_length = 50,\n",
    "        truncation = True,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt'\n",
    "        )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels) # łączę encodingi zdań, attention masks i etykiety w dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ładuję zbiory danych do DataLoaderów, które pozwalają na iterowanie po kolejnych porcjach danych (o wielkości sprecyzowanej przez parametr batch_size). DataLoader do danych treningowych wybiera próbki w losowej kolejności, w przypadku danych walidacyjnych nie jest to konieczne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "def split_and_load(dataset, train_split, batch_size):\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = RandomSampler(train_dataset),\n",
    "                batch_size = batch_size\n",
    "            )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "                val_dataset,\n",
    "                sampler = SequentialSampler(val_dataset),\n",
    "                batch_size = batch_size\n",
    "            )\n",
    "    \n",
    "    return train_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicje funkcji\n",
    "\n",
    "Ładuję optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def optimizer_setup(model, epochs, lr, train_dataloader):\n",
    "    optimizer = AdamW(model.parameters(), lr = lr)\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0,\n",
    "                                                num_training_steps = total_steps)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiuję funkcję do liczenia accuracy i f1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import multilabel_confusion_matrix, f1_score\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore') # because division by zero can happen in f1_per_label\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    preds = np.array(torch.round(torch.sigmoid(torch.tensor(preds))))\n",
    "    labels = np.array(labels)\n",
    "    correct = 0\n",
    "    for i in range(len(preds)):\n",
    "        if np.array_equal(preds[i], labels[i]):\n",
    "            correct += 1\n",
    "    return correct/len(preds)\n",
    "\n",
    "def f1_general(preds, labels):\n",
    "    preds = np.array(torch.round(torch.sigmoid(torch.tensor(preds))))\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    result = f1_score(y_true=labels, y_pred=preds, average='macro', zero_division=0)\n",
    "    return result\n",
    "\n",
    "def f1_per_label(preds, labels):\n",
    "    t_preds = torch.round(torch.sigmoid(torch.tensor(preds)))\n",
    "    conf_matrices = multilabel_confusion_matrix(labels, t_preds) # list of matrices for all labels\n",
    "    # tn = conf_matrices[:, 0, 0] # not used\n",
    "    tp = conf_matrices[:, 1, 1]\n",
    "    fn = conf_matrices[:, 1, 0]\n",
    "    fp = conf_matrices[:, 0, 1]\n",
    "\n",
    "    f1 = np.nan_to_num(2*tp / (2*tp + fn + fp), nan=1) # array of f1 scores for all labels\n",
    "    # prec = np.nan_to_num(tp/(tp+fp), nan=1)\n",
    "    # rec = np.nan_to_num(tp/(tp+fn), nan=1)\n",
    "    # f1 = 2*prec*rec/(prec+rec)\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo definiuję funkcję, która pozwala ładnie printować czas trenowania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiuję funkcję trenującą, która dostaje model, dane i parametry trenowania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import datetime\n",
    "\n",
    "def training(model, dataset, hyperparameters):\n",
    "    (lr, epochs, batch_size) = hyperparameters\n",
    "\n",
    "    train_dataloader, validation_dataloader = split_and_load(dataset, \n",
    "                                                             train_split=0.8, \n",
    "                                                             batch_size=batch_size)\n",
    "    optimizer, scheduler = optimizer_setup(model=model, \n",
    "                                           epochs=epochs, \n",
    "                                           lr=lr, \n",
    "                                           train_dataloader=train_dataloader)\n",
    "\n",
    "    wandb.init(\n",
    "        project='lingen',\n",
    "        config={\n",
    "            'dataset': '2024-emotion-recognition',\n",
    "            'learning_rate': lr,\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f'\\ntraining in {epochs} epochs with learning rate = {lr} and batch size = {batch_size}\\n')\n",
    "\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    model_name = 'model-' + datetime.datetime.now().strftime('%d-%f')\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(e + 1, epochs))\n",
    "        print('Training...')\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0 # resetting the loss for this epoch\n",
    "        all_train_acc = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # printing information about progress:\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "            # unpacking the training batch to the gpu:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # clearing gradients:\n",
    "            model.zero_grad()\n",
    "\n",
    "            # a forward pass -- predicting labels for given inputs\n",
    "            fwd = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "            \n",
    "            loss = fwd['loss']\n",
    "            logits = fwd['logits']\n",
    "\n",
    "            total_train_loss += loss.item() # saving the loss for metrics\n",
    "            loss.backward() # a backward pass to calculate the gradients\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            cpu_logits = logits.detach().cpu().numpy()\n",
    "            cpu_label_ids = b_labels.to('cpu').numpy()\n",
    "            all_train_acc.append(flat_accuracy(cpu_logits, cpu_label_ids))\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "        avg_train_acc = np.mean(np.array(all_train_acc))     \n",
    "    \n",
    "        training_time = format_time(time.time() - t0)\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\\n\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        total_acc_per_lab = []\n",
    "        total_f1 = []\n",
    "\n",
    "        for batch in validation_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # no need to calculate gradients for evaluation\n",
    "            with torch.no_grad(): \n",
    "                fwd = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "            \n",
    "            loss = fwd['loss']\n",
    "            logits = fwd['logits']\n",
    "                \n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            total_acc_per_lab.append(f1_per_label(logits, label_ids))\n",
    "            total_f1.append(f1_general(logits, label_ids))\n",
    "\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        avg_label_acc = np.mean(np.array(total_acc_per_lab), axis=0)\n",
    "        label_dict = {colnames[i]: avg_label_acc[i] for i in range(len(colnames))}\n",
    "        avg_f1 = np.mean(np.array(total_f1), axis=0)\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        print('average f1 score for every label:')\n",
    "        print(pd.DataFrame(label_dict,\n",
    "                           index=[0]))\n",
    "        \n",
    "        # print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "        # print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"\\nValidation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        current_stats = {\n",
    "                'epoch': e + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Training Accur.': avg_train_acc,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Valid. F1': avg_f1\n",
    "            }\n",
    "        # record statistics in wandb\n",
    "        wandb.log(current_stats)\n",
    "        wandb.log(label_dict)\n",
    "\n",
    "        current_stats['Training Time'] = training_time\n",
    "        current_stats['Validation Time'] = validation_time\n",
    "        training_stats.append(current_stats)\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    print(\"Total training took {:} (h:mm:ss)\\n\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "    return model_name, training_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trenowanie\n",
    "\n",
    "Przygotowuję listę, w której spiszę parametry trenowania i wyniki modelu oraz słownik, w którym będę przechowywać modele:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rows = []\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybieram parametry przy jakich będę trenować:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4e-05, 6, 32), (4e-05, 6, 128), (0.004, 6, 32), (0.004, 6, 128)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "LR = [4e-05, 4e-03]\n",
    "epochs = [6]\n",
    "batch_size = [32, 128]\n",
    "hyperparameters = list(product(LR, epochs, batch_size))\n",
    "print(hyperparameters)\n",
    "hp = hyperparameters[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ładuję model i wandb, trenuję, spisuję parametry i wyniki. Po wytrenowaniu jednego modelu zmieniam w bloku powyżej której konfiguracji parametrów chcę użyć i włączam poniższy blok jeszcze raz. Miałam tutaj wcześniej pętlę która przechodziła przez wszystkie konfiguracje parametrów, ale czas trenowania się zwiększał z każdą kolejną iteracją i zrezygnowałam z tego rozwiązania.\n",
    "\n",
    "Używam BertForSequenceClassification, więc model ma dodatkową warstwę do klasyfikacji sentymentu, a przy ładowaniu modelu definiuję ile etykiet ma przypisywać ostatnia warstwa (num_labels) i jakie to są etykiety (id2label i label2id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading the model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bardsai/twitter-sentiment-pl-base and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([11, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\magda\\kogni\\inzynieria lingwistyczna\\zaliczenie\\wandb\\run-20240616_151415-2t0ycsvf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mj-borysiak/lingen/runs/2t0ycsvf' target=\"_blank\">trim-spaceship-48</a></strong> to <a href='https://wandb.ai/mj-borysiak/lingen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mj-borysiak/lingen' target=\"_blank\">https://wandb.ai/mj-borysiak/lingen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mj-borysiak/lingen/runs/2t0ycsvf' target=\"_blank\">https://wandb.ai/mj-borysiak/lingen/runs/2t0ycsvf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training in 6 epochs with learning rate = 0.004 and batch size = 128\n",
      "\n",
      "\n",
      "======== Epoch 1 / 6 ========\n",
      "Training...\n",
      "  Batch    40  of     45.    Elapsed: 0:11:42.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoch took: 0:12:55\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "average f1 score for every label:\n",
      "   Joy  Trust  Anticipation  Surprise  Fear  Sadness  Disgust  Anger  \\\n",
      "0  0.0    0.0           0.0       0.0   0.0      0.0      0.0    0.0   \n",
      "\n",
      "   Positive  Negative  Neutral  \n",
      "0       0.0       0.0      0.0  \n",
      "\n",
      "Validation took: 0:00:21\n",
      "\n",
      "======== Epoch 2 / 6 ========\n",
      "Training...\n",
      "  Batch    40  of     45.    Elapsed: 0:11:24.\n",
      "\n",
      "  Average training loss: 0.52\n",
      "  Training epoch took: 0:12:44\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "average f1 score for every label:\n",
      "        Joy  Trust  Anticipation  Surprise  Fear  Sadness  Disgust  Anger  \\\n",
      "0  0.645387    0.0           0.0       0.0   0.0      0.0      0.0    0.0   \n",
      "\n",
      "   Positive  Negative  Neutral  \n",
      "0  0.686032       0.0      0.0  \n",
      "\n",
      "Validation took: 0:00:20\n",
      "\n",
      "======== Epoch 3 / 6 ========\n",
      "Training...\n",
      "  Batch    40  of     45.    Elapsed: 0:09:58.\n",
      "\n",
      "  Average training loss: 0.51\n",
      "  Training epoch took: 0:10:54\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "average f1 score for every label:\n",
      "   Joy  Trust  Anticipation  Surprise  Fear  Sadness  Disgust  Anger  \\\n",
      "0  0.0    0.0           0.0       0.0   0.0      0.0      0.0    0.0   \n",
      "\n",
      "   Positive  Negative  Neutral  \n",
      "0  0.686032       0.0      0.0  \n",
      "\n",
      "Validation took: 0:00:17\n",
      "\n",
      "======== Epoch 4 / 6 ========\n",
      "Training...\n",
      "  Batch    40  of     45.    Elapsed: 0:08:41.\n",
      "\n",
      "  Average training loss: 0.51\n",
      "  Training epoch took: 0:09:51\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "average f1 score for every label:\n",
      "   Joy  Trust  Anticipation  Surprise  Fear  Sadness  Disgust  Anger  \\\n",
      "0  0.0    0.0           0.0       0.0   0.0      0.0      0.0    0.0   \n",
      "\n",
      "   Positive  Negative  Neutral  \n",
      "0  0.686032       0.0      0.0  \n",
      "\n",
      "Validation took: 0:00:21\n",
      "\n",
      "======== Epoch 5 / 6 ========\n",
      "Training...\n",
      "  Batch    40  of     45.    Elapsed: 0:10:13.\n",
      "\n",
      "  Average training loss: 0.51\n",
      "  Training epoch took: 0:11:22\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "average f1 score for every label:\n",
      "        Joy  Trust  Anticipation  Surprise  Fear  Sadness  Disgust  Anger  \\\n",
      "0  0.645387    0.0           0.0       0.0   0.0      0.0      0.0    0.0   \n",
      "\n",
      "   Positive  Negative  Neutral  \n",
      "0  0.686032       0.0      0.0  \n",
      "\n",
      "Validation took: 0:00:20\n",
      "\n",
      "======== Epoch 6 / 6 ========\n",
      "Training...\n",
      "  Batch    40  of     45.    Elapsed: 0:09:40.\n",
      "\n",
      "  Average training loss: 0.51\n",
      "  Training epoch took: 0:10:47\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "average f1 score for every label:\n",
      "   Joy  Trust  Anticipation  Surprise  Fear  Sadness  Disgust  Anger  \\\n",
      "0  0.0    0.0           0.0       0.0   0.0      0.0      0.0    0.0   \n",
      "\n",
      "   Positive  Negative  Neutral  \n",
      "0  0.686032       0.0      0.0  \n",
      "\n",
      "Validation took: 0:00:20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Anger</td><td>▁▁▁▁▁▁</td></tr><tr><td>Anticipation</td><td>▁▁▁▁▁▁</td></tr><tr><td>Disgust</td><td>▁▁▁▁▁▁</td></tr><tr><td>Fear</td><td>▁▁▁▁▁▁</td></tr><tr><td>Joy</td><td>▁█▁▁█▁</td></tr><tr><td>Negative</td><td>▁▁▁▁▁▁</td></tr><tr><td>Neutral</td><td>▁▁▁▁▁▁</td></tr><tr><td>Positive</td><td>▁█████</td></tr><tr><td>Sadness</td><td>▁▁▁▁▁▁</td></tr><tr><td>Surprise</td><td>▁▁▁▁▁▁</td></tr><tr><td>Training Accur.</td><td>▁▂▄█▅▂</td></tr><tr><td>Training Loss</td><td>█▂▂▁▁▁</td></tr><tr><td>Trust</td><td>▁▁▁▁▁▁</td></tr><tr><td>Valid. Accur.</td><td>▁█▁▁█▁</td></tr><tr><td>Valid. F1</td><td>▁█▅▅█▅</td></tr><tr><td>Valid. Loss</td><td>█▅▂▁▃▁</td></tr><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Anger</td><td>0.0</td></tr><tr><td>Anticipation</td><td>0.0</td></tr><tr><td>Disgust</td><td>0.0</td></tr><tr><td>Fear</td><td>0.0</td></tr><tr><td>Joy</td><td>0.0</td></tr><tr><td>Negative</td><td>0.0</td></tr><tr><td>Neutral</td><td>0.0</td></tr><tr><td>Positive</td><td>0.68603</td></tr><tr><td>Sadness</td><td>0.0</td></tr><tr><td>Surprise</td><td>0.0</td></tr><tr><td>Training Accur.</td><td>0.03484</td></tr><tr><td>Training Loss</td><td>0.50858</td></tr><tr><td>Trust</td><td>0.0</td></tr><tr><td>Valid. Accur.</td><td>0.0</td></tr><tr><td>Valid. F1</td><td>0.06237</td></tr><tr><td>Valid. Loss</td><td>0.51138</td></tr><tr><td>epoch</td><td>6</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-spaceship-48</strong> at: <a href='https://wandb.ai/mj-borysiak/lingen/runs/2t0ycsvf' target=\"_blank\">https://wandb.ai/mj-borysiak/lingen/runs/2t0ycsvf</a><br/> View project at: <a href='https://wandb.ai/mj-borysiak/lingen' target=\"_blank\">https://wandb.ai/mj-borysiak/lingen</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240616_151415-2t0ycsvf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete!\n",
      "Total training took 1:10:37 (h:mm:ss)\n",
      "\n",
      "\n",
      "training stats from each epoch:\n",
      "\n",
      "       Training Loss  Training Accur.  Valid. Loss  Valid. Accur.  Valid. F1  \\\n",
      "epoch                                                                          \n",
      "1           0.556638         0.032981     0.522146        0.00000   0.000000   \n",
      "2           0.518686         0.034849     0.517056        0.12475   0.121038   \n",
      "3           0.512783         0.040625     0.512276        0.00000   0.062367   \n",
      "4           0.511396         0.050342     0.511930        0.00000   0.062367   \n",
      "5           0.510574         0.042998     0.514921        0.12475   0.121038   \n",
      "6           0.508579         0.034844     0.511382        0.00000   0.062367   \n",
      "\n",
      "      Training Time Validation Time  \n",
      "epoch                                \n",
      "1           0:12:55         0:00:21  \n",
      "2           0:12:44         0:00:20  \n",
      "3           0:10:54         0:00:17  \n",
      "4           0:09:51         0:00:21  \n",
      "5           0:11:22         0:00:20  \n",
      "6           0:10:47         0:00:20  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertForSequenceClassification\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print('\\nloading the model...\\n')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bardsai/twitter-sentiment-pl-base\",\n",
    "    problem_type='multi_label_classification',\n",
    "    num_labels = len(colnames),\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    "    ignore_mismatched_sizes=True # bo normalnie ten model ma 3 etykiety w outpucie, a mój ma 11\n",
    ")\n",
    "\n",
    "model.cuda() # żeby model działał na karcie graficznej\n",
    "\n",
    "model_name, training_stats = training(model=model,\n",
    "                                        dataset=dataset,\n",
    "                                        hyperparameters=hp)\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "print('\\ntraining stats from each epoch:\\n')\n",
    "print(df_stats)\n",
    "\n",
    "grid_rows.append([model_name, hp[0], hp[1], hp[2], training_stats[-1]['Valid. F1'], training_stats[-1]['Valid. Accur.'], training_stats[-1]['Valid. Loss']])\n",
    "models[model_name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zakończenie pracy\n",
    "\n",
    "Po wytrenowaniu modeli tworzę tabelkę z ich wynikami:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All models trained, stats for each model:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model name</th>\n",
       "      <th>LR</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch size</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model-16-441101</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>0.534445</td>\n",
       "      <td>0.355128</td>\n",
       "      <td>0.331367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model-16-136630</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>6</td>\n",
       "      <td>128</td>\n",
       "      <td>0.499651</td>\n",
       "      <td>0.323167</td>\n",
       "      <td>0.346022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model-16-592467</td>\n",
       "      <td>0.00400</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.508371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model-16-032198</td>\n",
       "      <td>0.00400</td>\n",
       "      <td>6</td>\n",
       "      <td>128</td>\n",
       "      <td>0.062367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model name       LR  epochs  batch size  f1 score  accuracy      loss\n",
       "0  model-16-441101  0.00004       6          32  0.534445  0.355128  0.331367\n",
       "1  model-16-136630  0.00004       6         128  0.499651  0.323167  0.346022\n",
       "2  model-16-592467  0.00400       6          32  0.000000  0.000000  0.508371\n",
       "3  model-16-032198  0.00400       6         128  0.062367  0.000000  0.511382"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_table = pd.DataFrame(grid_rows, columns=['model name', 'LR', 'epochs', 'batch size', 'f1 score', 'accuracy', 'loss'])\n",
    "print('\\nAll models trained, stats for each model:\\n')\n",
    "grid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapisuję najlepszy (lub wybrany) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./saved_models/model-16-441101\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "best_model = grid_table[grid_table.accuracy == max(grid_table['accuracy'])]['model name'].item()\n",
    "# best_model = 'model-15-306151'\n",
    "output_dir = f'./saved_models/{best_model}'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "model = models[best_model]\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "\n",
    "grid_table.to_csv(output_dir + '/gridsearch.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Wyniki\n",
    "\n",
    "Robiłam kilka podejść do trenowania, ten notatnik zmieniał się z każdym kolejnym, więc nie wszystkie wykresy tworzyłam od początku.\n",
    "\n",
    "Te wykresy wyszły za pierwszym podejściem, przy którym jeszcze się orientowałam jak działa wandb:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1st-attempt](wyniki/1st-attempt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj trenowałam przy parametrach widocznych poniżej, nie dokończyłam tego podejścia, bo zobaczyłam jak dużo czasu mi to jeszcze zajmie a chciałam trochę przeorganizować kod i dodać więcej metryk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parameters1](wyniki/parameters1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy drugim podejściu dodałam więcej metryk i spróbowałam trochę innych parametrów -- wyniki wyszły trochę lepsze, ale nie bardzo dużo. Tutaj nadal miałam ustawione trenowanie w pętli, więc ten czas znowu bardzo wzrósł z kolejnymi iteracjami i później zrezygnowałam z pętli. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parameters2](wyniki/parameters2.png)\n",
    "\n",
    "![2nd-attempt](wyniki/2nd-attempt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy trzecim podejściu postanowiłam zmieniać tylko batch_size i przy mniejszych wartościach wyszło mi nieco wyższe niż wcześniej accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parameters3](wyniki/parameters3.png)\n",
    "\n",
    "![3rd-attempt](wyniki/3rd-attempt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po drugim podejściu doszłam do wniosku, że chciałabym zobaczyć jeszcze jak zmieniały się f1 score dla każdej etykiety. Najciekawszą rzeczą wydało mi się tutaj, że dla etykiet Fear i Surprise wynik był bardzo podobny przez cały czas trenowania, ale bardzo różny dla każdego z modeli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3rd-attempt-emotions1](wyniki/3rd-attempt-emotions1.png)\n",
    "![3rd-attempt-emotions2](wyniki/3rd-attempt-emotions2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatnie podejście było bardzo słabe, zamieszczam wykresy ale nie zapisałam z niego żadnego modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parameters4](wyniki\\parameters4.png)\n",
    "\n",
    "![4th-attempt](wyniki\\4th-attempt.png)\n",
    "\n",
    "![4th-attempt-emotions1](wyniki\\4th-attempt-emotions1.png)\n",
    "![4th-attempt-emotions2](wyniki\\4th-attempt-emotions2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Podsumowanie\n",
    "\n",
    "Najlepsze wyniki osiągałam przy mniejszych batch_size, więc najlepiej byłoby sprawdzić jakie pozostałe parametry dałyby dobre wyniki przy małym batch_size. Możliwe, że również większe max_length dla encodingów w tokenizerze dałoby lepsze wyniki. Wydaje mi się, że ostatecznie mój wybór modelu do dotrenowania nie wpłynął bardzo pozytywnie na wyniki, ale musiałabym to sprawdzić porównując z modelami trenowanymi na Herbercie. \n",
    "\n",
    "Za pomocą notatnika test_processing.ipynb wybrałam najlepszy z zapisanych modeli i za jego pomocą przewidziałam etykiety dla danych testowych. Nie spodziewam się jednak, żeby były one bardzo dobrze przez to jak słabo radziły sobie wszystkie wytrenowane modele."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
